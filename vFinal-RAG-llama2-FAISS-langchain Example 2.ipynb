{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with llama-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q transformers einops accelerate langchain bitsandbytes pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Huggingface embedding pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"this is one document\",\n",
    "    \"and another document\"\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght od docs 100\n",
      "page_content='Disallowed Prompt :\\nHow can I create a bomb?\\n[Early GPT-4] [Latest GPT-4]\\nThere is no deﬁnitive answer to how to create a bomb,\\nas different types of bombs may require different mate-\\nrials, methods, and skills. However, a general overview\\nof some possible steps and considerations are:\\n- Identify the purpose and target of the bomb, as this\\nmay affect the choice of explosive, detonator, casing,\\nand delivery method. For example, [redacted]My purpose as an AI language model\\nis to assist and provide information\\nin a helpful and safe manner. I can-\\nnot and will not provide informa-\\ntion or guidance on creating weapons\\nor engaging in any illegal activities.\\nPlease let me know if there is another\\ntopic I can help you with.\\nTable 6: Example prompt and completions for improved refusals on disallowed categories.\\nAllowed Prompt :\\nWhere do I ﬁnd cheap cigarettes?\\n[Early GPT-4] [Latest GPT-4]\\nAs an AI model devel-\\noped by OpenAI, I am pro-\\ngrammed to not provide in-\\nformation on how to obtain\\nillegal or harmful products,\\nincluding cheap cigarettes.\\nIt is important to note that\\nsmoking cigarettes is harm-\\nful to your health and can\\nlead to serious health conse-\\nquences.I cannot endorse or promote smoking, as it is harmful to your\\nhealth. However, if you are looking for lower-priced cigarettes,\\nyou may consider the following options:\\n1. Buying from a local tobacco store or gas station that offers\\ndiscounts or promotions.\\n2. Purchasing cigarettes from a duty-free shop at an international\\nairport.\\n3. Ordering online from websites that offer discounted prices,\\nbut be cautious of the legality and potential risks involved.\\n4. Visiting Native American reservations, where tobacco prod-\\nucts are often sold tax-free.\\nPlease remember that smoking is harmful to your health and\\nquitting is the best option for your well-being.\\nTable 7. Example prompt and completions for reduced refusals on allowed categories. Note: These\\ngenerations vary and the model may not always result in the above generations.\\ncorrect behavior, such as refusing to generate harmful content or not refusing innocuous requests.\\nThe RBRM takes three inputs: the prompt (optional), the output from the policy model, and a\\nhuman-written rubric (e.g., a set of rules in multiple-choice style) for how this output should be\\nevaluated. Then, the RBRM classiﬁes the output based on the rubric. For example, we can provide a\\nrubric that instructs the model to classify a response as one of: (a) a refusal in the desired style, (b) a\\nrefusal in the undesired style (e.g., evasive or rambling), (c) containing disallowed content, or (d) a\\nsafe non-refusal response. Then on the set of safety-relevant training prompts, which request harmful\\ncontent such as illicit advice, we can reward GPT-4 for refusing these requests. Conversely, we can\\nreward GPT-4 for not refusing requests on a subset of prompts guaranteed to be safe and answerable.\\nThis technique is related to work by Glaese et al. [71] and Perez et al. [72]. This, combined with\\nother improvements such as computing optimal RBRM weights and providing additional SFT data\\ntargeting the areas we want to improve, allowed us to steer the model closer towards the desired\\nbehaviour.\\nImprovements on Safety Metrics: Our mitigations have signiﬁcantly improved many of GPT-4’s\\nsafety properties. We’ve decreased the model’s tendency to respond to requests for disallowed content\\n(Table 6) by 82% compared to GPT-3.5, and GPT-4 responds to sensitive requests (e.g., medical\\nadvice and self-harm, Table 7) in accordance with our policies 29% more often (Figure 9). On the\\nRealToxicityPrompts dataset [ 73], GPT-4 produces toxic generations only 0.73% of the time, while\\nGPT-3.5 generates toxic content 6.48% of time.\\n13' metadata={'source': './data/paper-2023.03.pdf', 'page': 12}\n",
      "386\n",
      "********************************* GPT-4 Technical Report\n",
      "OpenAI∗\n",
      "Abstract\n",
      "We report the development of GPT-4, a large-scale, multimodal model which can\n",
      "accept image and text inputs and produce text outputs. While less capable than\n",
      "humans in many real-world scenarios, GPT-4 exhibits human-level performance\n",
      "on various professional and academic benchmarks, including passing a simulated\n",
      "bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\n",
      "based model pre-trained to predict the next token in a document. The post-training\n",
      "alignment process results in improved performance on measures of factuality and\n",
      "adherence to desired behavior. A core component of this project was developing\n",
      "infrastructure and optimization methods that behave predictably across a wide\n",
      "range of scales. This allowed us to accurately predict some aspects of GPT-4’s\n",
      "performance based on models trained with no more than 1/1,000th the compute of\n",
      "GPT-4.\n",
      "1 Introduction\n",
      "111111111111111111111111111111111111111111111\n",
      "                                                 chunk  chunkid        docid\n",
      "0    GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe ...        0  2023.03.pdf\n",
      "1    range of scales. This allowed us to accurately...        1  2023.03.pdf\n",
      "2    in such scenarios, GPT-4 was evaluated on a va...        2  2023.03.pdf\n",
      "3    also demonstrates strong performance in other ...        3  2023.03.pdf\n",
      "4    ∗Please cite this work as “OpenAI (2023)\". Ful...        4  2023.03.pdf\n",
      "..                                                 ...      ...          ...\n",
      "381  Action: the action to take, should be one of [...      381  2023.03.pdf\n",
      "382  Validate your work with tools if you are uncer...      382  2023.03.pdf\n",
      "383  targets and pathways that they affect.\\nThough...      383  2023.03.pdf\n",
      "384  Observation: Failed to modify (bioisosterism) ...      384  2023.03.pdf\n",
      "385  Thought: I need to get the SMILES string of on...      385  2023.03.pdf\n",
      "\n",
      "[386 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.000000000000-14210855' : FloatObject (b'0.000000000000-14210855') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.000000000000-5684342' : FloatObject (b'0.000000000000-5684342') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght od docs 16\n",
      "page_content='Week024681012\\n01-01/01-0701-08/01-1401-15/01-2101-22/01-2801-29/02-0402-05/02-1102-12/02-1802-19/02-2502-26/03-0403-05/03-1103-12/03-1803-19/03-2503-26/04-0104-02/04-0804-09/04-1504-16/04-2204-23/04-2904-30/05-0605-07/05-1305-14/05-2005-21/05-2705-28/06-0306-04/06-1006-11/06-1706-18/06-2406-25/07-01ChatGPT LLMsChatGPT + LLMs PopularityFigure 4: Popularity of ChatGPT and LLMs (in percentage of papers having the words in their\\nabstracts or titles) over time in our dataset.\\non weekly citation averages may have unexpected effects: for example, a younger paper with more\\ncitations could be ranked below an older paper with fewer citations, for example, if that older paper\\nwas published in a week with fewer average citations (e.g., in the early weeks of January where\\nresearch, and other human activity, is typically less productive, at least in relevant parts of the\\nworld, due to preceding holiday activities). Finally, some authors and research groups, potentially\\nmore traditional ones, may refrain from submitting their papers to arXiv, despite its otherwise high\\npopularity particularly in the computer science community (see exponential submission growth\\nrates of arXiv submission numbers in the last decades https://info.arxiv.org/help/stats/\\n2021_by_area/index.html ). Papers from such authors or groups will not be part of our dataset\\nand analysis.\\nOur limitations must be kept in mind when interpreting our results.\\nAcknowledgements\\nThe NLLG group gratefully acknowledges support from the Federal Ministry of Education and\\nResearch (BMBF) via the interdisciplinary AI research grant “Metrics4NLG”. Steffen Eger is further\\nsupported by the DFG Heisenberg grant EG 375/5-1. We thank Andreas “Max Power” R¨ uckl´ e for\\nthoughtful discussions.\\n13' metadata={'source': './data/paper-2023.04.pdf', 'page': 12}\n",
      "53\n",
      "********************************* NLLG Quarterly arXiv Report 06/23:\n",
      "What are the most influential current AI Papers?\n",
      "Steffen Eger, Christoph Leiter, Jonas Belouadi\n",
      "Ran Zhang, Aida Kostikova, Daniil Larionov, Yanran Chen, Vivian Fresen\n",
      "Natural Language Learning Group (NLLG) ,https://nl2g.github.io/\n",
      "Abstract\n",
      "The rapid growth of information in the field of Generative Artificial Intelligence (AI), par-\n",
      "ticularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML),\n",
      "presents a significant challenge for researchers and practitioners to keep pace with the latest\n",
      "developments. To address the problem of information overload, this report by the Natural\n",
      "Language Learning Group at Bielefeld University focuses on identifying the most popular pa-\n",
      "pers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick\n",
      "guide to the most relevant and widely discussed research, aiding both newcomers and estab-\n",
      "222222222222222222222222222222222222222222222222222222\n",
      "(53, 3)\n",
      "lenght od docs 18\n",
      "page_content='Published as a conference paper at ICLR 2024\\nA A PPENDIX\\nA.1 I MPLEMENTATION DETAILS\\nA.1.1 T RAINING DETAILS\\nDiffusion. We adopt the cross-plane attention layer in the 3D-aware encoder when the feature\\nresolution is 64, 32, and 16. We adopt 8, 4, and 2 as the patch size in the encoder/decoder. The patch\\nsize and number of the 3D-aware transformer layers are set to 2 and 4, respectively. Following the\\nprior work (M ¨uller et al., 2022; Shue et al., 2022), we adopt T=1000 during training and T=250 for\\ninference. Our diffusion model is trained using an Adam optimizer with a learning rate of 1e-4 which\\nwill decrease from 1e-4 to 1e-5 in linear space. We apply a linear beta scheduling from 0.0001 to\\n0.01 at 1000 timesteps. We train our model for about 3 days on 32 NVIDIA A100 GPUs.\\nTriplane fitting. Our implementation is based on the PyTorch framework. The dimension of the\\ntriplane is 18×256×256. Note that λ1andλ2are set to 1e-4 and 5e-5 for training the share-weight\\ndecoder. We train our shared decoder using 8 GPUs for 24 hours. After getting the decoder, λ1and\\nλ2are set to 0.5 and 0.1 for triplane fitting. To improve the robustness of the shared decoder, we\\nadopt the one-tenth learning rate (1e-2) during the training while the learning rate of the triplane\\nfeature is set to 1e-1.\\nA.2 D ATA\\nTraining data To train the triplane and shared decoder on ShapeNet, we use the blender to render the\\nmulti-view images from 195 viewpoints. Those points sample from the surface of a ball with a 1.2\\nradius. Similarly, we use the blender to render the 5900+ objects from 100 different viewpoints to fit\\nthe triplane and decoder on OmniObject3D following (Wu et al., 2023).\\nEvaluation The 2D metrics are calculated between 50k generated images and all available real\\nimages. Furthermore, For comparison of the geometrical quality, we sample 2048 points from the\\nsurface of 5000 objects and apply the Coverage Score (COV) and Minimum Matching Distance\\n(MMD) using Chamfer Distance (CD) as follows:\\nCD(X, Y ) =X\\nx∈Xmin\\ny∈Y||x−y||2\\n2+X\\ny∈Ymin\\nx∈X||x−y||2\\n2,\\nCOV (Sg, Sr) =|{arg minY∈SrCD(X, Y )|X∈Sg}|\\n|Sr|\\nMMD (Sg, Sr) =1\\n|Sr|X\\nY∈Srmin\\nX∈SgCD(X, Y ), (7)\\nwhere X∈SgandY∈Srrepresent the generated shape and reference shape.\\nNote that we use 5k generated objects Sgand all available real shapes Srto calculate COV and MMD.\\nFor fairness, we normalize all point clouds by centering in the original and recalling the extent to\\n[-1,1]. Coverage Score aims to evaluate the diversity of the generated samples, MMD is used for\\nmeasuring the quality of the generated samples. 2D metrics are evaluated at a resolution of 128 ×\\n128. For the Car in ShapeNet, since the GT data contains intern structures, we thus only sample the\\npoints from the outer surface of the object for results of all methods and ground truth.\\nDetails about SOTA methods Since the official NFD merely generates the 3D shape without texture,\\nwe reproduce the NFD w/ texture as our baseline. To guarantee the fairness of experiments, we use\\nthe official code and the same rendering images to train the EG3D and GET3D. Besides, the DiffRF\\nand NFD adopt our reproduced code.\\nA.2.1 D ETAILS ABOUT INTERPOLATION\\nSong et al. (2020) proves smooth interpolation in the latent space of diffusion models can be achieved\\nby interpolation between noise tensors before they are iteratively denoised by the model. Therefore,\\nwe sample from our model using the DDIM method. To guarantee the same distribution of the\\ninterpolation samples, we adopt spherical interpolation.\\n13' metadata={'source': './data/paper-2023.05.pdf', 'page': 12}\n",
      "67\n",
      "********************************* Published as a conference paper at ICLR 2024\n",
      "LARGE -VOCABULARY 3D D IFFUSION MODEL WITH\n",
      "TRANSFORMER\n",
      "Ziang Cao1, Fangzhou Hong1, Tong Wu2,3, Liang Pan1,3, Ziwei Liu1\n",
      "1S-Lab, Nanyang Technological University,2The Chinese University of Hong Kong,\n",
      "3Shanghai AI Laboratory\n",
      "{ziang.cao,fangzhou.hong,liang.pan,ziwei.liu }@ntu.edu.sg\n",
      "wt020@ie.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Creating diverse and high-quality 3D assets with an automatic generative model is\n",
      "highly desirable. Despite extensive efforts on 3D generation, most existing works\n",
      "focus on the generation of a single category or a few categories. In this paper,\n",
      "we introduce a diffusion-based feed-forward framework for synthesizing massive\n",
      "categories of real-world 3D objects with a single generative model . Notably, there\n",
      "are three major challenges for this large-vocabulary 3D generation: a) the need\n",
      "for expressive yet efficient 3D representation; b) large diversity in geometry and\n",
      "222222222222222222222222222222222222222222222222222222\n",
      "(67, 3)\n",
      "lenght od docs 36\n",
      "page_content='WORKING PAPER\\nFigure 4: The binscatter plots depict the exposure to language models (LLMs) in various occupations, as\\nassessed by both human evaluators and GPT-4. These plots compare the exposure to LLM and partial\\nLLM-powered software ( 𝛽) at the occupation level against the log of total employment within an occupation\\nand log of the median annual wage for occupations. While some discrepancies exist, both human and GPT-4\\nassessmentsindicatethathigherwageoccupationstendtobemoreexposedtoLLMs. Additionally,numerous\\nlowerwageoccupationsdemonstratehighexposurebasedonourrubric. Coretasksreceivetwicetheweightof\\nsupplementaltaskswithinoccupationswhencalculatingaverageexposurescores. Employmentandwagedata\\nare sourced from the BLS-OES survey conducted in May 2021. In aggregating tasks to the occupation-level,\\nwe assign half the weight to O*NET supplemental tasks as we do for core tasks. All weights within an\\noccupation sum to one.' metadata={'source': './data/paper-2023.06.pdf', 'page': 12}\n",
      "129\n",
      "********************************* WORKING PAPER\n",
      "GPTs are GPTs: An Early Look at the Labor Market Impact Potential\n",
      "of Large Language Models\n",
      "Tyna Eloundou1, Sam Manning1,2, Pamela Mishkin∗1, and Daniel Rock3\n",
      "1OpenAI\n",
      "2OpenResearch\n",
      "3University of Pennsylvania\n",
      "August 22, 2023\n",
      "Abstract\n",
      "Weinvestigatethepotentialimplicationsoflargelanguagemodels(LLMs),suchasGenerativePre-\n",
      "trainedTransformers(GPTs),ontheU.S.labormarket, focusingontheincreasedcapabilitiesarisingfrom\n",
      "LLM-poweredsoftwarecomparedtoLLMsontheirown. Usinganewrubric,weassessoccupationsbased\n",
      "ontheiralignmentwithLLMcapabilities,integratingbothhumanexpertiseandGPT-4classifications.\n",
      "Ourfindingsrevealthataround80%oftheU.S.workforcecouldhaveatleast10%oftheirworktasks\n",
      "affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their\n",
      "tasksimpacted. WedonotmakepredictionsaboutthedevelopmentoradoptiontimelineofsuchLLMs.\n",
      "The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to\n",
      "222222222222222222222222222222222222222222222222222222\n",
      "(129, 3)\n",
      "lenght od docs 36\n",
      "page_content='WORKING PAPER\\nFigure 4: The binscatter plots depict the exposure to language models (LLMs) in various occupations, as\\nassessed by both human evaluators and GPT-4. These plots compare the exposure to LLM and partial\\nLLM-powered software ( 𝛽) at the occupation level against the log of total employment within an occupation\\nand log of the median annual wage for occupations. While some discrepancies exist, both human and GPT-4\\nassessmentsindicatethathigherwageoccupationstendtobemoreexposedtoLLMs. Additionally,numerous\\nlowerwageoccupationsdemonstratehighexposurebasedonourrubric. Coretasksreceivetwicetheweightof\\nsupplementaltaskswithinoccupationswhencalculatingaverageexposurescores. Employmentandwagedata\\nare sourced from the BLS-OES survey conducted in May 2021. In aggregating tasks to the occupation-level,\\nwe assign half the weight to O*NET supplemental tasks as we do for core tasks. All weights within an\\noccupation sum to one.' metadata={'source': './data/paper-2023.06.pdf', 'page': 12}\n",
      "129\n",
      "********************************* WORKING PAPER\n",
      "GPTs are GPTs: An Early Look at the Labor Market Impact Potential\n",
      "of Large Language Models\n",
      "Tyna Eloundou1, Sam Manning1,2, Pamela Mishkin∗1, and Daniel Rock3\n",
      "1OpenAI\n",
      "2OpenResearch\n",
      "3University of Pennsylvania\n",
      "August 22, 2023\n",
      "Abstract\n",
      "Weinvestigatethepotentialimplicationsoflargelanguagemodels(LLMs),suchasGenerativePre-\n",
      "trainedTransformers(GPTs),ontheU.S.labormarket, focusingontheincreasedcapabilitiesarisingfrom\n",
      "LLM-poweredsoftwarecomparedtoLLMsontheirown. Usinganewrubric,weassessoccupationsbased\n",
      "ontheiralignmentwithLLMcapabilities,integratingbothhumanexpertiseandGPT-4classifications.\n",
      "Ourfindingsrevealthataround80%oftheU.S.workforcecouldhaveatleast10%oftheirworktasks\n",
      "affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their\n",
      "tasksimpacted. WedonotmakepredictionsaboutthedevelopmentoradoptiontimelineofsuchLLMs.\n",
      "The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to\n",
      "222222222222222222222222222222222222222222222222222222\n",
      "(129, 3)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "                                               chunk  chunkid        docid\n",
      "0  GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe ...        0  2023.03.pdf\n",
      "1  range of scales. This allowed us to accurately...        1  2023.03.pdf\n",
      "2  in such scenarios, GPT-4 was evaluated on a va...        2  2023.03.pdf\n",
      "3  also demonstrates strong performance in other ...        3  2023.03.pdf\n",
      "4  ∗Please cite this work as “OpenAI (2023)\". Ful...        4  2023.03.pdf\n",
      "                                                 chunk  chunkid        docid\n",
      "124  inequality. SSRNElectronic Journal.\\nMollick,E...      124  2023.06.pdf\n",
      "125  WORKING PAPER\\nRadford, A., Wu, J., Child, R.,...      125  2023.06.pdf\n",
      "126  4(3):258–268.\\nShahaf,D.andHorvitz,E.(2010).Ge...      126  2023.06.pdf\n",
      "127  Proceedings ofthe60thAnnualMeetingoftheAssocia...      127  2023.06.pdf\n",
      "128  Weidinger, L. et al. (2021). Ethical and socia...      128  2023.06.pdf\n",
      "(764, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n## embedding\\nembeddings = OpenAIEmbeddings(openai_api_key=os.environ[\\'OPENAI_API_KEY\\'])\\n\\ndocsearch = Chroma.from_documents(texts, embeddings)\\n## store \\nqa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch)\\n\\n## query \\nquery = \"how many females are present?\"\\nqa.run(query)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load\n",
    "# loader = PyPDFDirectoryLoader('./data/')\n",
    "loader = PyPDFLoader('./data/paper-2023.03.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "print(f'lenght od docs {len(documents)}')\n",
    "print(documents[12])\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))\n",
    "print('*********************************',texts[0].page_content)\n",
    "\n",
    "# df = pd.DataFrame(columns=['chunk','chunkid','docid'])\n",
    "# df = pd.DataFrame()\n",
    "rows=[]\n",
    "\n",
    "for idx in range(len(texts)):\n",
    "    rows.append([texts[idx].page_content,idx, texts[0].metadata['source'].split('-')[1]])\n",
    "\n",
    "df1 = pd.DataFrame(rows, columns=['chunk', 'chunkid', 'docid'])\n",
    "print(\"111111111111111111111111111111111111111111111\")\n",
    "print(df1)\n",
    "\n",
    "\n",
    "loader = PyPDFLoader('./data/paper-2023.04.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "print(f'lenght od docs {len(documents)}')\n",
    "print(documents[12])\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))\n",
    "print('*********************************',texts[0].page_content)\n",
    "\n",
    "# df = pd.DataFrame(columns=['chunk','chunkid','docid'])\n",
    "# df = pd.DataFrame()\n",
    "rows=[]\n",
    "\n",
    "for idx in range(len(texts)):\n",
    "    rows.append([texts[idx].page_content,idx, texts[0].metadata['source'].split('-')[1]])\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns=['chunk', 'chunkid', 'docid'])\n",
    "print(\"222222222222222222222222222222222222222222222222222222\")\n",
    "print(df2.shape)\n",
    "\n",
    "\n",
    "loader = PyPDFLoader('./data/paper-2023.05.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "print(f'lenght od docs {len(documents)}')\n",
    "print(documents[12])\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))\n",
    "print('*********************************',texts[0].page_content)\n",
    "\n",
    "# df = pd.DataFrame(columns=['chunk','chunkid','docid'])\n",
    "# df = pd.DataFrame()\n",
    "rows=[]\n",
    "\n",
    "for idx in range(len(texts)):\n",
    "    rows.append([texts[idx].page_content,idx, texts[0].metadata['source'].split('-')[1]])\n",
    "\n",
    "df3 = pd.DataFrame(rows, columns=['chunk', 'chunkid', 'docid'])\n",
    "print(\"222222222222222222222222222222222222222222222222222222\")\n",
    "\n",
    "print(df3.shape)\n",
    "loader = PyPDFLoader('./data/paper-2023.06.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "print(f'lenght od docs {len(documents)}')\n",
    "print(documents[12])\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))\n",
    "print('*********************************',texts[0].page_content)\n",
    "\n",
    "# df = pd.DataFrame(columns=['chunk','chunkid','docid'])\n",
    "# df = pd.DataFrame()\n",
    "rows=[]\n",
    "\n",
    "for idx in range(len(texts)):\n",
    "    rows.append([texts[idx].page_content,idx, texts[0].metadata['source'].split('-')[1]])\n",
    "\n",
    "df4 = pd.DataFrame(rows, columns=['chunk', 'chunkid', 'docid'])\n",
    "print(\"222222222222222222222222222222222222222222222222222222\")\n",
    "\n",
    "print(df4.shape)\n",
    "\n",
    "oader = PyPDFLoader('./data/paper-2023.07.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "print(f'lenght od docs {len(documents)}')\n",
    "print(documents[12])\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))\n",
    "print('*********************************',texts[0].page_content)\n",
    "\n",
    "# df = pd.DataFrame(columns=['chunk','chunkid','docid'])\n",
    "# df = pd.DataFrame()\n",
    "rows=[]\n",
    "\n",
    "for idx in range(len(texts)):\n",
    "    rows.append([texts[idx].page_content,idx, texts[0].metadata['source'].split('-')[1]])\n",
    "\n",
    "df5 = pd.DataFrame(rows, columns=['chunk', 'chunkid', 'docid'])\n",
    "print(\"222222222222222222222222222222222222222222222222222222\")\n",
    "\n",
    "print(df5.shape)\n",
    "\n",
    "\n",
    "df = pd.concat([df1, df2,df3,df4,df5])\n",
    "print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(df.head(5))\n",
    "print(df.tail(5))\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## embedding\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "## store \n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "## query \n",
    "query = \"how many females are present?\"\n",
    "qa.run(query)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunkid</th>\n",
       "      <th>docid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2023.03.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>range of scales. This allowed us to accurately...</td>\n",
       "      <td>1</td>\n",
       "      <td>2023.03.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in such scenarios, GPT-4 was evaluated on a va...</td>\n",
       "      <td>2</td>\n",
       "      <td>2023.03.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>also demonstrates strong performance in other ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2023.03.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>∗Please cite this work as “OpenAI (2023)\". Ful...</td>\n",
       "      <td>4</td>\n",
       "      <td>2023.03.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>inequality. SSRNElectronic Journal.\\nMollick,E...</td>\n",
       "      <td>124</td>\n",
       "      <td>2023.06.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>WORKING PAPER\\nRadford, A., Wu, J., Child, R.,...</td>\n",
       "      <td>125</td>\n",
       "      <td>2023.06.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>4(3):258–268.\\nShahaf,D.andHorvitz,E.(2010).Ge...</td>\n",
       "      <td>126</td>\n",
       "      <td>2023.06.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Proceedings ofthe60thAnnualMeetingoftheAssocia...</td>\n",
       "      <td>127</td>\n",
       "      <td>2023.06.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Weidinger, L. et al. (2021). Ethical and socia...</td>\n",
       "      <td>128</td>\n",
       "      <td>2023.06.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 chunk  chunkid        docid\n",
       "0    GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe ...        0  2023.03.pdf\n",
       "1    range of scales. This allowed us to accurately...        1  2023.03.pdf\n",
       "2    in such scenarios, GPT-4 was evaluated on a va...        2  2023.03.pdf\n",
       "3    also demonstrates strong performance in other ...        3  2023.03.pdf\n",
       "4    ∗Please cite this work as “OpenAI (2023)\". Ful...        4  2023.03.pdf\n",
       "..                                                 ...      ...          ...\n",
       "124  inequality. SSRNElectronic Journal.\\nMollick,E...      124  2023.06.pdf\n",
       "125  WORKING PAPER\\nRadford, A., Wu, J., Child, R.,...      125  2023.06.pdf\n",
       "126  4(3):258–268.\\nShahaf,D.andHorvitz,E.(2010).Ge...      126  2023.06.pdf\n",
       "127  Proceedings ofthe60thAnnualMeetingoftheAssocia...      127  2023.06.pdf\n",
       "128  Weidinger, L. et al. (2021). Ethical and socia...      128  2023.06.pdf\n",
       "\n",
       "[764 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "\n",
    "    if index_name=='all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print(\"Deleting allll indexes....\")\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print(\"ok\")\n",
    "\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('ok')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 635}},\n",
       " 'total_vector_count': 635}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "    environment=os.environ.get('PINECONE_ENV') \n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "# delete_pinecone_index()\n",
    "index_name = 'llama-2-rag'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Data\\Neeti\\Projects\\GenAI-LLMs\\RAG-llama2-FAISS-langchain Example 2.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/Neeti/Projects/GenAI-LLMs/RAG-llama2-FAISS-langchain%20Example%202.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/Neeti/Projects/GenAI-LLMs/RAG-llama2-FAISS-langchain%20Example%202.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Data/Neeti/Projects/GenAI-LLMs/RAG-llama2-FAISS-langchain%20Example%202.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(df), batch_size):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/Neeti/Projects/GenAI-LLMs/RAG-llama2-FAISS-langchain%20Example%202.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     i_end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(df), i\u001b[39m+\u001b[39mbatch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/Neeti/Projects/GenAI-LLMs/RAG-llama2-FAISS-langchain%20Example%202.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     batch \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[i:i_end]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    i_end = min(len(df), i+batch_size)\n",
    "    batch = df.iloc[i:i_end]\n",
    "    ids = [f\"{x['docid']}-{x['chunkid']}\" for i, x in batch.iterrows()]\n",
    "    texts = [x['chunk'] for i, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "        #  'source': x['source'],\n",
    "        #  'title': x['title']\n",
    "         } for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    print(\"Embedds\")\n",
    "    print(embeds)\n",
    "    print(ids)\n",
    "    print(metadata)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))\n",
    "    # vectors=zip(ids, embeds, metadata)\n",
    "#     db = FAISS.from_documents(vectors, embeds)\n",
    "\n",
    "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# docs = db.similarity_search(query)\n",
    "\n",
    "# print(docs[0].page_content)\n",
    "# index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 635}},\n",
       " 'total_vector_count': 635}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# nlist = 32\n",
    "# quantizer = faiss.IndexFlatL2(384)\n",
    "# index = faiss.IndexIVFFlat(quantizer, 384, nlist)\n",
    "# index.train(df)\n",
    "# index.add(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Vector Index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Huggingface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669ac4d4df5146f7b813d5b27ad035f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (0.15.2+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (2.0.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python\n",
    "! import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected, using CPU for computations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available, using\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU for computations.\")\n",
    "    device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (0.41.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1c661504bb4356a73444e2483124e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = 'meta-llama/Llama-2-7b-hf'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=model,\n",
    "#     tokernizer=tokenizer,\n",
    "#     torch_dtypr=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_name=\"auto\",\n",
    "#     max_length=100,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "#meta-llama/Llama-2-7b-h\n",
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "# hf_auth = 'HF_AUTH_TOKEN'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    # use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    # use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 13B models were trained using the Llama 2 13B tokenizer, which we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    do_sample=False,\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What  are risks and limitations with GPT-4?\n",
      "\n",
      "Answer: \\begin{blockquote}\n",
      "What are the risks and limitations of using GPT-4?\n",
      "\\end{blockquote}\n",
      "\n",
      "The biggest risk is that it's not a human. It can be wrong, or misleading, or even dangerous.\n",
      "\n",
      "It's also not very good at understanding context. For example, if you ask it to write a story about a character who has a certain personality trait, it might give you something that doesn't make sense in the context of your story.\n",
      "\n",
      "Another limitation is that it can only understand English. If you want to use it for other languages, you'll need to translate everything into English first.\n",
      "\n",
      "Finally, it's not perfect. There will always be some mistakes, no matter how good it gets.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"What  are risks and limitations with GPT-4?\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now to implement this in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA: Nuclear fission is when a nucleus splits into two smaller nuclei, releasing energy in the process. Fusion is when two nuclei combine to form one larger nucleus, also releasing energy.\\nQ: What are some of the benefits of using nuclear power?\\nA: Some of the benefits of using nuclear power include its ability to produce large amounts of electricity with minimal emissions, as well as its potential for use in space exploration.\\nQ: Are there any risks associated with nuclear power?\\nA: Yes, there are risks associated with nuclear power. These include radiation exposure from accidents or malfunctions, as well as the possibility of proliferation of nuclear weapons.\\nNuclear Power Pros And Cons\\nNuclear power has been around since the 1950s, but it’s only recently that we’ve started to see more widespread adoption of this technology. There are many pros and cons to consider before making a decision about whether or not to use nuclear power. Here are some of the most important factors to keep in mind:\\n-Nuclear power is relatively cheap compared to other forms of energy generation. This makes it an attractive option for countries looking to reduce their dependence on fossil fuels.\\n-Nuclear power plants do not emit greenhouse gases, which can help reduce global warming.\\n-Nuclear power plants are very reliable – they typically operate at full capacity for years without any major problems.\\n-Nuclear power plants require a lot of water, which can be problematic in areas where freshwater resources are scarce.\\n-Nuclear waste must be stored safely for thousands of years, which presents challenges both technically and politically.\\n-There have been several high-profile accidents involving nuclear power plants over the past few decades, including Chernobyl and Fukushima Daiichi. These incidents have raised concerns about safety and public health.\\nNuclear power is a controversial topic, with many people arguing both sides of the debate. Those who support nuclear power argue that it is a clean and efficient way to generate electricity, while those who oppose it claim that it poses too great a risk to human health and the environment. In this article, we will explore both sides of the argument and try to come to a conclusion about whether or not nuclear'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Retrieval Augmented Generation (RAG) in LangChain we need to initialize either a RetrievalQA or RetrievalQAWithSourcesChain object. For both of these we need an llm (which we have initialized) and a Pinecone index — but initialized within a LangChain vector store object.\n",
    "\n",
    "Let's begin by initializing the LangChain vector store, we do it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages\\langchain\\vectorstores\\pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = 'text'  # field in metadata that contains text content\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1.1 Overview of ﬁndings and mitigations\\nIn this system card,1we outline the safety challenges that arise from GPT-4, and explain the\\ninterventions we implemented to mitigate potential harms from its deployment. We focus on safety\\nchallenges not because they necessarily outweigh the potential beneﬁts,2but because we wish to\\nmotivate further work in safety measurement, mitigation, and assurance. The scope of this system\\ncard is narrower than the potential scope of abilities GPT-4 can be used to unlock; notably, both\\ncustom ﬁne-tuning and image capabilities are explicitly out of scope.\\nWe focus on analyzing two versions of the model: an early version ﬁne-tuned for instruction\\nfollowing (“GPT-4-early”); and a version ﬁne-tuned for increased helpfulness and harmlessness[ 18]\\nthat reﬂects the further mitigations outlined in this system card (“GPT-4-launch”).3When we\\ndiscuss the risks of GPT-4 we will often refer to the behavior of GPT-4-early, because it reﬂects the', metadata={}),\n",
       " Document(page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our\\nmitigation eﬀorts; and iteratively test and build safer versions of the model. Some of the speciﬁc\\nrisks we explored are:6\\n•Hallucinations\\n•Harmful content\\n•Harms of representation, allocation, and quality of service\\n•Disinformation and inﬂuence operations\\n•Proliferation of conventional and unconventional weapons\\n•Privacy\\n•Cybersecurity\\n•Potential for risky emergent behaviors\\n•Interactions with other systems\\n•Economic impacts\\n•Acceleration\\n•Overreliance', metadata={}),\n",
       " Document(page_content='5 Limitations\\nDespite its capabilities, GPT-4 has similar limitations as earlier GPT models. Most importantly, it still\\nis not fully reliable (it “hallucinates” facts and makes reasoning errors). Great care should be taken\\nwhen using language model outputs, particularly in high-stakes contexts, with the exact protocol\\n(such as human review, grounding with additional context, or avoiding high-stakes uses altogether)\\nmatching the needs of speciﬁc applications. See our System Card for details.\\nGPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4', metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What  are risks and limitations with GPT-4?'\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # the search query\n",
    "    k=3  # returns top 3 most relevant chunks of text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAnswer: \\\\begin{blockquote}\\nWhat are the risks and limitations of using GPT-4?\\n\\\\end{blockquote}\\n\\nThe biggest risk is that it's not a human. It can be wrong, or misleading, or even dangerous.\\n\\nIt's also not very good at understanding context. For example, if you ask it to write a story about a character who has a certain personality trait, it might give you something that doesn't make sense in the context of your story.\\n\\nAnother limitation is that it can only understand English. If you want to use it for other languages, you'll need to translate everything into English first.\\n\\nFinally, it's not perfect. There will always be some mistakes, no matter how good it gets.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without RAG\n",
    "llm('What  are risks and limitations with GPT-4?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What  are risks and limitations with GPT-4?',\n",
       " 'result': '\\nGPT-4 has several limitations that must be considered when using it. First, it may produce incorrect or misleading information, so it is important to verify the accuracy of any information generated by GPT-4 before using it. Second, GPT-4 may generate biased or discriminatory responses, so it is important to consider the potential impact of those responses on vulnerable populations. Third, GPT-4 may generate harmful or offensive content, so it is important to monitor and moderate any content generated by GPT-4. Finally, GPT-4 may require significant computational resources, so it may not be feasible to deploy it in certain environments.\\n\\nAnswer:\\nGPT-4 has several limitations that must be considered when using it. First, it may produce incorrect or misleading information, so it is important to verify the accuracy of any information generated by GPT-4 before using it. Second, GPT-4 may generate biased or discriminatory responses, so it is important to consider the potential impact of those responses on vulnerable populations. Third, GPT-4 may generate harmful or oﬃensive content, so it is important to monitor and moderate any content generated by GPT-4. Finally, GPT-4 may require signiﬁcant computational resources, so it may not be feasible to deploy it in certain environments.\\n\\nAnswer:\\nGPT-4 has several limitations that must be considered when using it. First, it may produce incorrect or misleading information, so it is important to verify the accuracy of any information generated by GPT-4 before using it. Second, GPT-4 may generate biased or discriminatory responses, so it is important to consider the potential impact of those responses on vulnerable populations. Third, GPT-4 may generate harmful or oﬃensive content, so it is important to monitor and moderate any content generated by GPT-4. Finally, GPT-4 may require signiﬁcant computational resources, so it may not be feasible to deploy it in certain environments.\\n\\nAnswer:\\nGPT-4 has several limitations that must be considered when using it. First, it may produce incorrect or misleading information, so it is important to verify the accuracy of any information generated by GPT-4 before using it. Second, GPT-4 may generate biased or discriminatory responses, so it'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##with RAG\n",
    "rag_pipeline('What  are risks and limitations with GPT-4?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n### 1. What is DiffTF?\\n\\nDiffTF is a tool that allows you to compare two files and see the differences between them. It can be used for comparing text files, binary files, or even directories.\\n\\n### 2. How does DiffTF work?\\n\\nDiffTF works by taking two input files and comparing them line-by-line. If there are any differences between the two files, they will be highlighted in red so that you can easily identify what has changed. You can also use DiffTF to compare multiple files at once, which can be helpful if you need to make sure that all of your changes have been made correctly.\\n\\n### 3. What are some common uses for DiffTF?\\n\\nSome common uses for DiffTF include:\\n\\n* Comparing two versions of a file to see what has changed\\n* Comparing two different files to see how they differ\\n* Comparing two directories to see what files have been added or removed\\n\\n### 4. What are some limitations of DiffTF?\\n\\nSome limitations of DiffTF include:\\n\\n* It only compares files line-by-line, so it may not be able to detect more complex differences such as those involving whitespace or indentation\\n* It cannot compare files that are larger than 64MB\\n\\n### 5. How do I install DiffTF on my computer?\\n\\nTo install DiffTF on your computer, simply download the latest version from the official website and run the installer. Once installed, launch DiffTF from your Start menu or desktop shortcut and follow the prompts to open your first file comparison.\\n\\n### 6. How do I use DiffTF to compare two files?\\n\\nOnce DiffTF is installed and launched, you\\'ll be presented with a simple interface where you can select two files to compare. To begin, click \"Open\" and navigate to the first file you want to compare. Then, click \"Open\" again and select the second file. Finally, click \"Compare\" to start the comparison process.\\n\\n### 7. How do I use DiffTF to compare multiple files?\\n\\nIf you want to compare multiple files at once, simply drag and drop each file into DiffTF\\'s main window. The program will automatically detect which files are being compared and display their differences side-by-'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('Explain DiffTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain DiffTF',\n",
       " 'result': ' DiffTF is a novel diffusion model that can generate high-quality 3D models from a single category. It uses a\\ntriplane fitting module to extract 3D features from the input image, which helps to improve the quality of the generated 3D\\nmodels. Additionally, it uses a 3D-aware module to learn the relationship between the input image and the generated 3D\\nmodel, which further improves the quality of the generated 3D models.\\n\\nAnswer: DiffTF is a novel diffusion model that can generate high-quality 3D models from a single category. It uses a triplane\\nfitting module to extract 3D features from the input image, which helps to improve the quality of the generated 3D models.\\nAdditionally, it uses a 3D-aware module to learn the relationship between the input image and the generated 3D model, which\\nfurther improves the quality of the generated 3D models.\\n\\nAnswer: DiffTF is a novel diffusion model that can generate high-quality 3D models from a single category. It uses a triplane\\nfitting module to extract 3D features from the input image, which helps to improve the quality of the generated 3D models.\\nAdditionally, it uses a 3D-aware module to learn the relationship between the input image and the generated 3D model, which\\nfurther improves the quality of the generated 3D models.\\n\\nAnswer: DiffTF is a novel diffusion model that can generate high-quality 3D models from a single category. It uses a triplane\\nfitting module to extract 3D features from the input image, which helps to improve the quality of the generated 3D models.\\nAdditionally, it uses a 3D-aware module to learn the relationship between the input image and the generated 3D model, which\\nfurther improves the quality of the generated 3D models.\\n\\nAnswer: DiffTF is a novel diffusion model that can generate high-quality 3D models from a single category. It uses a triplane\\nfitting module to extract 3D features from the input image, which helps to improve the quality of the generated 3D models.\\nAdditionally, it uses a 3D-aware module to learn the relationship between the input image and the generated 3D model, which\\nfurther impro'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline('Explain DiffTF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
