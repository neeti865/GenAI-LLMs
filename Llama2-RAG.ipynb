{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Package Installations including required model load, framework , embedding packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q transformers einops accelerate langchain bitsandbytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (3.12.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (2023.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/b8/d4/ce436660098b2f456e2b8fdf76d4f33cbc3766c874c4aa2f772c7a5e943f/ipywidgets-8.1.0-py3-none-any.whl.metadata\n",
      "  Using cached ipywidgets-8.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.7 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.7 from https://files.pythonhosted.org/packages/8e/d4/d31b12ac0b87e8cc9fdb6ea1eb6596de405eaaa2f25606aaa755d0eebbc0/widgetsnbextension-4.0.8-py3-none-any.whl.metadata\n",
      "  Using cached widgetsnbextension-4.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.7 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab-widgets~=3.0.7 from https://files.pythonhosted.org/packages/74/5e/2475ac62faf2e342b2bf20b8d8e375f49400ecb38f52e4e0a7557eb1cedb/jupyterlab_widgets-3.0.8-py3-none-any.whl.metadata\n",
      "  Using cached jupyterlab_widgets-3.0.8-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Using cached ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
      "Using cached widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.0 jupyterlab-widgets-3.0.8 widgetsnbextension-4.0.8\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login into Hugging face and use the access token from your account to get the Llama2 into the n/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5d41fd67d40dcb38720b6463f402e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 30.7/86.0 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 86.0/86.0 kB 970.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sentence_transformers) (4.33.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sentence_transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sentence_transformers) (1.25.2)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/96/cf/a714a655266229b51eb2bda117f15275f12457887f165f3c1cc58ab502f1/scikit_learn-1.3.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/70/03/485f73046134400ea25d3cb178c5e6728f9b165f79d09638ecb44ee0e9b1/scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.1/59.1 kB ? eta 0:00:00\n",
      "Collecting nltk (from sentence_transformers)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.3/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.8/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.4/1.5 MB 10.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 10.7 MB/s eta 0:00:00\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.3)\n",
      "Collecting click (from nltk->sentence_transformers)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->sentence_transformers)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from torchvision->sentence_transformers) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/9.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.0/9.2 MB 12.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.4/9.2 MB 11.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.5/9.2 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.0/9.2 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.5/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.0/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.2 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.1/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.6/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.1/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.6/9.2 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.0/9.2 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.4/9.2 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.2 MB 10.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.8/9.2 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.2/9.2 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "   ---------------------------------------- 0.0/44.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/44.0 MB 10.5 MB/s eta 0:00:05\n",
      "    --------------------------------------- 1.0/44.0 MB 10.6 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.5/44.0 MB 10.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 2.1/44.0 MB 10.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.6/44.0 MB 10.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 3.0/44.0 MB 11.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 3.6/44.0 MB 10.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 4.1/44.0 MB 11.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.6/44.0 MB 10.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 5.2/44.0 MB 11.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.7/44.0 MB 11.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 6.2/44.0 MB 11.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.6/44.0 MB 11.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 7.2/44.0 MB 11.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 7.7/44.0 MB 11.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 8.2/44.0 MB 11.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 8.7/44.0 MB 11.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 9.2/44.0 MB 11.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 9.7/44.0 MB 11.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 10.3/44.0 MB 11.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 10.8/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 11.3/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 11.8/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 12.4/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 12.9/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 13.4/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 13.9/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 14.4/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 14.9/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 15.4/44.0 MB 10.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 16.0/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 16.5/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.0/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.5/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 18.0/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 18.5/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.0/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.6/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 20.0/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 20.6/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 21.1/44.0 MB 10.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 21.6/44.0 MB 11.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 22.1/44.0 MB 10.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 22.7/44.0 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.1/44.0 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.7/44.0 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 24.1/44.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 24.7/44.0 MB 11.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 25.1/44.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.6/44.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.9/44.0 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.3/44.0 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 26.6/44.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 26.9/44.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 27.3/44.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 27.9/44.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 28.0/44.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 28.1/44.0 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 28.7/44.0 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 28.9/44.0 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 29.3/44.0 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 29.3/44.0 MB 9.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 29.8/44.0 MB 8.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.3/44.0 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 30.8/44.0 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 31.3/44.0 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 31.7/44.0 MB 8.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.2/44.0 MB 8.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.5/44.0 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.8/44.0 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.5/44.0 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.0/44.0 MB 8.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.4/44.0 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 35.0/44.0 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.5/44.0 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 35.8/44.0 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.3/44.0 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.7/44.0 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.2/44.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.6/44.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.8/44.0 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.3/44.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.7/44.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.3/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.6/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.9/44.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.4/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 40.8/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.1/44.0 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.6/44.0 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.0/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.5/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.0/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.0/44.0 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py): started\n",
      "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125960 sha256=a4ad897b00d9127472de0b33f91d13edcd8c40fd4774d878789d99d4b5f0109a\n",
      "  Stored in directory: c:\\users\\nisha\\appdata\\local\\pip\\cache\\wheels\\62\\f2\\10\\1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, threadpoolctl, scipy, joblib, click, scikit-learn, nltk, sentence_transformers\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 scikit-learn-1.3.0 scipy-1.11.2 sentence_transformers-2.2.2 sentencepiece-0.1.99 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPU being used or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected, using CPU for computations.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available, using\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU for computations.\")\n",
    "    device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898d02b0aa09415c8ab10abab2d9a0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c97fa6fbd3447af9e080de21a3c4440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4b455cc5f7400ebf5577aea549b87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b6b5e795d34bf7ac1fb82d2b6519e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fc49a721f546c594d20087b9332e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a4cd8c97a642b4acc34eae76304bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff74ed1b8494593aa5ee9964f75db93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908a588a7d6643fca54a373e94516850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402c24446e8b45f8a9224a801f09d91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc397f6be7848088bd2253543c5a554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee999b5148a42048a3ccea418ceae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae7a3785ad64f43ab68f8276c3c50ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734f1468287b4e00b361ebd244067bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d510e865ea4971affb684c49f9e550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device':device},\n",
    "    encode_kwargs={'device':device, 'batch_size':32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings total 6 eith a dimensionality of 384\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "\n",
    "\" Swin-Unet Existing medical image segmentation methods mainly rely on fully convolutional neural network (FCNN) with U-shaped structure [3,4,5]. \",\n",
    "\"The typical U-shaped network, U-Net [3], consists of a symmetric Encoder-Decoder with skip connections. \",\n",
    "\"In the encoder, a series of convolutional layers and continuous down-sampling layers are used to extract deep features with large receptive fields.\",\n",
    "\"Then, the decoder up-samples the extracted deep features to the input resolution for pixel-level semantic prediction, and the high-resolution features\", \n",
    "\" of different scale from the encoder are fused with skip connections to alleviate the loss of spatial information caused by down-sampling. \" , \n",
    "\"With such an elegant structural design, U-Net has achieved great success in a variety of medical imaging applications.\" \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(docs)\n",
    "\n",
    "print(f\"embeddings total {len(embeddings)} eith a dimensionality of {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build out your vectopr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Obtaining dependency information for pinecone-client from https://files.pythonhosted.org/packages/98/17/3675b83dca0a032d2750bf04fbfdf78a6e46fa3056eefc2574cdd14661d9/pinecone_client-2.2.2-py3-none-any.whl.metadata\n",
      "  Using cached pinecone_client-2.2.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (6.0.1)\n",
      "Collecting loguru>=0.5.0 (from pinecone-client)\n",
      "  Obtaining dependency information for loguru>=0.5.0 from https://files.pythonhosted.org/packages/19/a9/4e91197b121a41c640367641a510fd9a05bb7a3259fc9678ee2976c8fd00/loguru-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading loguru-0.7.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (4.7.1)\n",
      "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
      "  Obtaining dependency information for dnspython>=2.0.0 from https://files.pythonhosted.org/packages/f6/b4/0a9bee52c50f226a3cbfb54263d02bb421c7f2adc136520729c2c689c1e5/dnspython-2.4.2-py3-none-any.whl.metadata\n",
      "  Using cached dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (2.0.4)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from pinecone-client) (1.25.2)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (0.4.6)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru>=0.5.0->pinecone-client)\n",
      "  Using cached win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\miniconda3\\envs\\llama2huggingface\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "Using cached pinecone_client-2.2.2-py3-none-any.whl (179 kB)\n",
      "Using cached dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "Downloading loguru-0.7.1-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/61.4 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 51.2/61.4 kB 525.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 61.4/61.4 kB 544.6 kB/s eta 0:00:00\n",
      "Installing collected packages: win32-setctime, dnspython, loguru, pinecone-client\n",
      "Successfully installed dnspython-2.4.2 loguru-0.7.1 pinecone-client-2.2.2 win32-setctime-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import pinecone\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "pinecone.init(\n",
    "    api_key = os.environ.get('PINECONE_API_KEY'),\n",
    "    environment= os.environ.get('PINECONE_ENV')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is to initialize the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['askswinunet']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "\n",
    "    if index_name=='all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print(\"Deleting allll indexes....\")\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print(\"ok\")\n",
    "\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('ok')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you need to delete index : I have a basica free tier pinecone account that allows only 1 index at a given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting allll indexes....\n",
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_pinecone_index()\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index llama2-rag.....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "index_name = 'llama2-rag'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    print(f'Creating index {index_name}.....')\n",
    "    pinecone.create_index(index_name,dimension=1536,metric=\"cosine\", pods=1, pod_type=\"p1.x2\")\n",
    "    print('Done')\n",
    "else:\n",
    "    print(f'index {index_name} already exists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connect to the index\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/Llama-2-7b-hf'\n",
    "tokernizer = AutoTokenizer.from_pretrained(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac946b9541741a29841462251afbbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9a5f2b2415401c95b4befd1a74b9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d840c15e3664427a99139c69f097dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe8ec0050b041a7825eccdd820f2da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5244baeec2dd4339bebb6765769fe1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model,\n",
    "    tokernizer=tokernizer,\n",
    "    torch_dtypr=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_name=\"auto\",\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokernizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline,model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "        Write concise summary of the following text delimited by triple backquotes. Return your response in bullet points which covers the key points of the text. ''' {text}''' \n",
    "        BULLET POINT SuMMARY : \n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "text = \"\"\"\n",
    "Swin-Unet\n",
    "1 Technische Universit¨at M¨unchen, M¨unchen, Germany 2 Fudan University, Shanghai, China 3 Huawei Technologies, Shanghai, China\n",
    "Abstract. In the past few years, convolutional neural networks (CNNs)\n",
    "have achieved milestones in medical image analysis. Especially, the deep\n",
    "neural networks based on U-shaped architecture and skip-connections\n",
    "have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn\n",
    "global and long-range semantic information interaction well due to the\n",
    "locality of convolution operation. In this paper, we propose Swin-Unet,\n",
    "which is a Unet-like pure Transformer for medical image segmentation.\n",
    "The tokenized image patches are fed into the Transformer-based Ushaped Encoder-Decoder architecture with skip-connections for localglobal semantic feature learning. Specifically, we use hierarchical Swin\n",
    "Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct downsampling and up-sampling of the inputs and outputs by 4×, experiments on multi-organ and cardiac segmentation tasks demonstrate that\n",
    "the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly\n",
    "available at https://github.com/HuCaoFighting/Swin-Unet.\n",
    "1 Introduction\n",
    "Benefiting from the development of deep learning, computer vision technology\n",
    "has been widely used in medical image analysis. Image segmentation is an important part of medical image analysis. In particular, accurate and robust medical\n",
    "image segmentation can play a cornerstone role in computer-aided diagnosis and\n",
    "image-guided clinical surgery [1,2].\n",
    "*Corresponding author † Work done as an intern in Huawei Technologies\n",
    "arXiv:2105.05537v1 [eess.IV] 12 May 2021\n",
    "2 Hu Cao et al.\n",
    "Existing medical image segmentation methods mainly rely on fully convolutional neural network (FCNN) with U-shaped structure [3,4,5]. The typical\n",
    "U-shaped network, U-Net [3], consists of a symmetric Encoder-Decoder with\n",
    "skip connections. In the encoder, a series of convolutional layers and continuous down-sampling layers are used to extract deep features with large receptive\n",
    "fields. Then, the decoder up-samples the extracted deep features to the input\n",
    "resolution for pixel-level semantic prediction, and the high-resolution features\n",
    "of different scale from the encoder are fused with skip connections to alleviate\n",
    "the loss of spatial information caused by down-sampling. With such an elegant\n",
    "structural design, U-Net has achieved great success in a variety of medical imaging applications. Following this technical route, many algorithms such as 3D\n",
    "U-Net [6], Res-UNet [7], U-Net++ [8] and UNet3+ [9] have been developed for\n",
    "image and volumetric segmentation of various medical imaging modalities. The\n",
    "excellent performance of these FCNN-based methods in cardiac segmentation,\n",
    "organ segmentation and lesion segmentation proves that CNN has a strong ability of learning discriminating features.\n",
    "Currently, although the CNN-based methods have achieved excellent performance in the field of medical image segmentation, they still cannot fully meet\n",
    "the strict requirements of medical applications for segmentation accuracy. Image\n",
    "segmentation is still a challenge task in medical image analysis. Since the intrinsic locality of convolution operation, it is difficult for CNN-based approaches to\n",
    "learn explicit global and long-range semantic information interaction [2]. Some\n",
    "studies have tried to address this problem by using atrous convolutional layers [10,11], self-attention mechanisms [12,13], and image pyramids [14]. However,\n",
    "these methods still have limitations in modeling long - range dependencies. Recently, inspired by Transformer’s great success in the nature language processing\n",
    "(NLP) domain [15], researchers have tried to bring Transformer into the vision\n",
    "domain [16]. In [17], vision transformer (ViT) is proposed to perform the image recognition task. Taking 2D image patches with positional embeddings as\n",
    "inputs and pre-training on large dataset, ViT achieved comparable performance\n",
    "with the CNN-based methods. Besides, data-efficient image transformer (DeiT)\n",
    "is presented in [18], which indicates that Transformer can be trained on mid-size\n",
    "datasets and that a more robust Transformer can be obtained by combining it\n",
    "with the distillation method. In [19], a hierarchical Swin Transformer is developed. Take Swin Transformer as vision backbone, the authors of [19] achieved\n",
    "state-of-the-art performance on Image classification, object detection and semantic segmentation. The success of ViT, DeiT and Swin Transformer in image\n",
    "recognition task demonstrates the potential for Transformer to be applied in the\n",
    "vision domain.\n",
    "Motivated by the Swin Transformer’s [19] success, we propose Swin-Unet\n",
    "to leverage the power of Transformer for 2D medical image segmentation in\n",
    "this work. To our best knowledge, Swin-Unet is a first pure Transformer-based\n",
    "U-shaped architecture that consists of encoder, bottleneck, decoder, and skip\n",
    "connections. Encoder, bottleneck and decoder are all built based on Swin Transformer block [19]. The input medical images are split into non-overlapping image\n",
    "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 3\n",
    "patches. Each patch is treated as a token and fed into the Transformer-based\n",
    "encoder to learn deep feature representations. The extracted context features are\n",
    "then up-sampled by the decoder with patch expanding layer, and fused with the\n",
    "multi-scale features from the encoder via skip connections, so as to restore the\n",
    "spatial resolution of the feature maps and further perform segmentation prediction. Extensive experiments on multi-organ and cardiac segmentation datasets\n",
    "indicate that the proposed method has excellent segmentation accuracy and robust generalization ability. Concretely, our contributions can be summarized as:\n",
    "(1) Based on Swin Transformer block, we build a symmetric Encoder-Decoder\n",
    "architecture with skip connections. In the encoder, self-attention from local to\n",
    "global is realized; in the decoder, the global features are up-sampled to the input resolution for corresponding pixel-level segmentation prediction. (2) A patch\n",
    "expanding layer is developed to achieve up-sampling and feature dimension increase without using convolution or interpolation operation. (3) It is found in\n",
    "the experiment that skip connection is also effective for Transformer, so a pure\n",
    "Transformer-based U-shaped Encoder-Decoder architecture with skip connection\n",
    "is finally constructed, named Swin-Unet.\n",
    "2 Related work\n",
    "CNN-based methods : Early medical image segmentation methods are mainly\n",
    "contour-based and traditional machine learning-based algorithms [20,21]. With\n",
    "the development of deep CNN, U-Net is proposed in [3] for medical image segmentation. Due to the simplicity and superior performance of the U-shaped\n",
    "structure, various Unet-like methods are constantly emerging, such as Res-UNet [7],\n",
    "Dense-UNet [22], U-Net++ [8] and UNet3+ [9]. And it is also introduced into\n",
    "the field of 3D medical image segmentation, such as 3D-Unet [6] and V-Net [23].\n",
    "At present, CNN-based methods have achieved tremendous success in the field\n",
    "of medical image segmentation due to its powerful representation ability.\n",
    "Vision transformers : Transformer was first proposed for the machine translation task in [15]. In the NLP domain, the Transformer-based methods have\n",
    "achieved the state-of-the-art performance in various tasks [24]. Driven by Transformer’s success, the researchers introduced a pioneering vision transformer (ViT)\n",
    "in [17], which achieved the impressive speed-accuracy trade-off on image recognition task. Compared with CNN-based methods, the drawback of ViT is that it\n",
    "requires pre-training on its own large dataset. To alleviate the difficulty in training ViT, Deit [18] describes several training strategies that allow ViT to train\n",
    "well on ImageNet. Recently, several excellent works have been done baed on\n",
    "ViT [25,26,19]. It is worth mentioning that an efficient and effective hierarchical\n",
    "vision Transformer, called Swin Transformer, is proposed as a vision backbone\n",
    "in [19]. Based on the shifted windows mechanism, Swin Transformer achieved\n",
    "the state-of-the-art performance on various vision tasks including image classification, object detection and semantic segmentation. In this work, we attempt to\n",
    "use Swin Transformer block as basic unit to build a U-shaped Encoder-Decoder\n",
    "4 Hu Cao et al.\n",
    "architecture with skip connections for medical image segmentation, thus providing a benchmark comparison for the development of Transformer in the medical\n",
    "image field.\n",
    "Self-attention/Transformer to complement CNNs : In recent years, researchers have tried to introduce self-attention mechanism into CNN to improve\n",
    "the performance of the network [13]. In [12], the skip connections with additive\n",
    "attention gate are integrated in U-shaped architecture to perform medical image\n",
    "segmentation. However, this is still the CNN-based method. Currently, some efforts are being made to combine CNN and Transformer to break the dominance\n",
    "of CNNs in medical image segmentation [2,27,1]. In [2], the authors combined\n",
    "Transformer with CNN to constitute a strong encoder for 2D medical image segmentation. Similar to [2], [27] and [28] use the complementarity of Transformer\n",
    "and CNN to improve the segmentation capability of the model. Currently, various combinations of Transformer with CNN are applied in multi-modal brain\n",
    "tumor segmentation [29] and 3D medical image segmentation [1,30]. Different\n",
    "from the above methods, we try to explore the application potential of pure\n",
    "Transformer in medical image segmentation.\n",
    "3 Method\n",
    "3.1 Architecture overview\n",
    "The overall architecture of the proposed Swin-Unet is presented in Figure. 1.\n",
    "Swin-Unet consists of encoder, bottleneck, decoder and skip connections. The\n",
    "basic unit of Swin-Unet is Swin Transformer block [19]. For the encoder, to\n",
    "transform the inputs into sequence embeddings, the medical images are split into\n",
    "non-overlapping patches with patch size of 4 × 4. By such partition approach,\n",
    "the feature dimension of each patch becomes to 4 × 4 × 3 = 48. Furthermore, a\n",
    "linear embedding layer is applied to projected feature dimension into arbitrary\n",
    "dimension (represented as C). The transformed patch tokens pass through several\n",
    "Swin Transformer blocks and patch merging layers to generate the hierarchical\n",
    "feature representations. Specifically, patch merging layer is responsible for downsampling and increasing dimension, and Swin Transformer block is responsible\n",
    "for feature representation learning. Inspired by U-Net [3], we design a symmetric transformer-based decoder. The decoder is composed of Swin Transformer\n",
    "block and patch expanding layer. The extracted context features are fused with\n",
    "multiscale features from encoder via skip connections to complement the loss\n",
    "of spatial information caused by down-sampling. In contrast to patch merging\n",
    "layer, a patch expanding layer is specially designed to perform up-sampling. The\n",
    "patch expanding layer reshapes feature maps of adjacent dimensions into a large\n",
    "feature maps with 2× up-sampling of resolution. In the end, the last patch expanding layer is used to perform 4× up-sampling to restore the resolution of the\n",
    "feature maps to the input resolution (W ×H), and then a linear projection layer\n",
    "is applied on these up-sampled features to output the pixel-level segmentation\n",
    "predictions. We would elaborate each block in the following\n",
    "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 5\n",
    "Fig. 1. The architecture of Swin-Unet, which is composed of encoder, bottleneck, decoder and skip connections. Encoder, bottleneck and decoder are all constructed based\n",
    "on swin transformer block.\n",
    "3.2 Swin Transformer block\n",
    "Different from the conventional multi-head self attention (MSA) module, swin\n",
    "transformer block [19] is constructed based on shifted windows. In Figure. 2,\n",
    "two consecutive swin transformer blocks are presented. Each swin transformer\n",
    "block is composed of LayerNorm (LN) layer, multi-head self attention module,\n",
    "residual connection and 2-layer MLP with GELU non-linearity. The windowbased multi-head self attention (W-MSA) module and the shifted window-based\n",
    "multi-head self attention (SW-MSA) module are applied in the two successive\n",
    "6 Hu Cao et al.\n",
    "Fig. 2. Swin transformer block.\n",
    "transformer blocks, respectively. Based on such window partitioning mechanism,\n",
    "continuous swin transformer blocks can be formulated as:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.run(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
